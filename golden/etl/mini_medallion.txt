ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: Starting parse_ask_safe_expression
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: Node structure: "(ask_safe_expression (ask_safe_kv_pair value: (string_literal)) (ask_safe_kv_pair value: (string_literal)) (ask_safe_kv_pair value: (float_literal)))"
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: Processing child node: kind=ask_safe_kv_pair, text=Some("user: \"Produce a short, factual executive summary for the selected tickets\";")
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: Found ask_safe_kv_pair node
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: ask_safe_kv_pair key=user, value_kind=string_literal
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: Processing child node: kind=ask_safe_kv_pair, text=Some("validation: \"basic\";")
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: Found ask_safe_kv_pair node
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: ask_safe_kv_pair key=validation, value_kind=string_literal
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: Processing child node: kind=ask_safe_kv_pair, text=Some("confidence_threshold: 0.8;")
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: Found ask_safe_kv_pair node
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: ask_safe_kv_pair key=confidence_threshold, value_kind=float_literal
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: Final validation_strategy: Some("basic")
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: Final confidence_threshold: Some(0.8)
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: Final cross_reference_models: []
ğŸ›¡ï¸ DEBUG ASK_SAFE_PROCESSOR: Final attributes: []
ğŸ›¡ï¸ DEBUG HIR->LEXIR: ask_safe attributes before extraction: {}
ğŸ›¡ï¸ DEBUG HIR->LEXIR: validation_strategy: Some("basic"), confidence_threshold: Some(0.8), max_attempts: None
ğŸ›¡ï¸ DEBUG HIR->LEXIR: cross_reference_models: []
ğŸ“„ Vector Memory System initialized with SQLite backend
ğŸ”§ set_default_model: Setting default model to 'gpt-4'
ğŸ”§ LlmAdapter: Setting default model to 'gpt-4'
âœ… set_default_model: Default model updated successfully
ğŸ“Š load_csv: Loading CSV file 'samples/data/triage/tickets.csv'
[DEBUG] [data_operation] EVENT: Reading CSV
ğŸ“Š Loading data from: samples/data/triage/tickets.csv
   Loaded 6 rows with 4 columns
âœ… load_csv: CSV loaded successfully
[DEBUG] [data_operation] COMPLETED in 15.269042ms
ğŸ” Filtering dataset with condition: priority == high
   Filter applied. Execution deferred (lazy evaluation)
ğŸ” Taking 5 rows from dataset
   Limit applied. Execution deferred (lazy evaluation)
[INFO] [ask_operation] EVENT: ask_merge: calling LLM
ğŸ” DEBUG call_llm: model=Some("gpt-4"), user_prompt=Some("Consolidate these responses using synthesize strategy:\n\nSummarize top highâ€‘priority tickets in two bullet points.\n---\nList owners and a short action for each ticket.\n---\nIdentify one risk and one quick win.\n---\nIdentifier(\"\")\n\nConsolidated result:")
ğŸ” DEBUG: Checking cache for key: "model=gpt-4;temp=0.3;sys=You are an expert at consolidating information.;user=Consolidate these responses using synthesize strategy:\n\nSummarize top highâ€‘priority tickets in two bullet points.\n---\nList owners and a short action for each ticket.\n---\nIdentify one risk and one quick win.\n---\nIdentifier(\"\")\n\nConsolidated result:;schema="
[INFO] [llm_call] EVENT: Calling OpenAI API
[INFO] [llm_call] EVENT: OpenAI call successful
[INFO] [llm_call] COMPLETED in 5.8412965s
[INFO] [ask_operation] COMPLETED in 5.868363625s
ğŸ›¡ï¸ DEBUG: Executing AskSafe instruction with validation
ğŸ›¡ï¸ Starting ANTI-HALLUCINATION validation for prompt: Produce a short, factual executive summary for the selected tickets
ğŸ”„ Validation attempt 1/3
ğŸ” DEBUG call_llm: model=None, user_prompt=Some("Produce a short, factual executive summary for the selected tickets")
âš ï¸  No default model configured, using fallback
ğŸ” DEBUG: Checking cache for key: "model=simulated;temp=0.3;sys=;user=Produce a short, factual executive summary for the selected tickets;schema="
ğŸ“Š Token usage (SIMULATED):
   - Model: simulated (SIMULATED)
   - Prompt tokens: 18
   - Completion tokens: 60
   - Total tokens: 78
ğŸ’° Estimated cost (simulated): $0.002340
ğŸ” Running BASIC validation...
âœ… Validation PASSED with confidence: 1.00
ğŸ›¡ï¸ AskSafe completed: valid=true, confidence=1.00
ğŸ’¾ save_json: Saving data to 'output/etl_top_high.json'
[DEBUG] [data_operation] EVENT: Saving JSON
âœ… save_json: Data saved successfully
[DEBUG] [data_operation] COMPLETED in 3.43925ms
ğŸ¯ ask_multioutput: Generating response with 1 output files
ğŸ“ Prompt: Create a brief markdown report for the filtered tickets
ğŸ“ Output files: ["etl_report.md"]
[INFO] [ask_operation] EVENT: ask_multioutput: calling LLM
ğŸ” DEBUG call_llm: model=Some("gpt-4"), user_prompt=Some("Create a brief markdown report for the filtered tickets")
ğŸ” DEBUG: Checking cache for key: "model=gpt-4;temp=0.2;sys=You can emit files using a tool-call like JSON: {\"tool_calls\":[{\"name\":\"emit_file\",\"arguments\":{\"name\":string,\"mime\":string,\"b64\":string}}...]}. Do not include prose when emitting files.;user=Create a brief markdown report for the filtered tickets;schema="
[INFO] [llm_call] EVENT: Calling OpenAI API
[INFO] [llm_call] EVENT: OpenAI call successful
[INFO] [llm_call] COMPLETED in 14.205147042s
âœ… ask_multioutput: Generated response with 1 files
[INFO] [ask_operation] COMPLETED in 14.229152042s
ğŸ’¾ Saved file (streamed): output/etl_report.md (40 bytes, text/markdown)
ETL miniâ€‘medallion completed
