üìÑ Vector Memory System initialized with SQLite backend
üîß set_default_model: Setting default model to 'simulated'
üîß LlmAdapter: Setting default model to 'simulated'
‚úÖ set_default_model: Default model updated successfully
üîó session_start: Creating session 'demo_session' with model simulated
üí¨ session_ask: Session 'demo_session' - User: User: Provide an overview of the project scope.
üîç DEBUG call_llm: model=Some("gpt-3.5-turbo"), user_prompt=Some("User: Provide an overview of the project scope.")
üîç DEBUG: Checking cache for key: "model=gpt-3.5-turbo;temp=0.7;sys=You are in session 'demo_session'. Maintain conversation context.;user=User: Provide an overview of the project scope.;schema="
[INFO] [llm_call] EVENT: Calling OpenAI API
[INFO] [llm_call] EVENT: OpenAI call successful
[INFO] [llm_call] COMPLETED in 1.148253875s
üí¨ session_ask: Session 'demo_session' - User: User: What are the key milestones?
üîç DEBUG call_llm: model=Some("gpt-3.5-turbo"), user_prompt=Some("User: What are the key milestones?")
üîç DEBUG: Checking cache for key: "model=gpt-3.5-turbo;temp=0.7;sys=You are in session 'demo_session'. Maintain conversation context.;user=User: What are the key milestones?;schema="
[INFO] [llm_call] EVENT: Calling OpenAI API
[INFO] [llm_call] EVENT: OpenAI call successful
[INFO] [llm_call] COMPLETED in 1.541063917s
üß† session_summarize: Generating intelligent summary for session 'demo_session'
üîç DEBUG call_llm: model=None, user_prompt=Some("Please summarize this conversation session:\n\nDemo conversation for session demo_session\nUser: Hello, I'm testing the Lexon language features.\nAssistant: Great! Lexon is a revolutionary LLM-native programming language.\nUser: Can you explain the key features?\nAssistant: Lexon supports async/await, multimodal requests, session management, and anti-hallucination validation.\nUser: That sounds impressive. How does the session system work?\nAssistant: Sessions allow persistent context across multiple interactions, with automatic summarization and compression.\nUser: Perfect, let's test the context window management now.\n\nSummary requirements:\n- Length: medium\n- Focus: comprehensive\n- Style: professional\n\nProvide a comprehensive yet concise summary that captures the key points, decisions, and outcomes.")
‚ö†Ô∏è  No default model configured, using fallback
üîç DEBUG: Checking cache for key: "model=simulated;temp=0.3;sys=You are an expert session summarizer. Create a medium summary with comprehensive focus in a professional style.;user=Please summarize this conversation session:\n\nDemo conversation for session demo_session\nUser: Hello, I'm testing the Lexon language features.\nAssistant: Great! Lexon is a revolutionary LLM-native programming language.\nUser: Can you explain the key features?\nAssistant: Lexon supports async/await, multimodal requests, session management, and anti-hallucination validation.\nUser: That sounds impressive. How does the session system work?\nAssistant: Sessions allow persistent context across multiple interactions, with automatic summarization and compression.\nUser: Perfect, let's test the context window management now.\n\nSummary requirements:\n- Length: medium\n- Focus: comprehensive\n- Style: professional\n\nProvide a comprehensive yet concise summary that captures the key points, decisions, and outcomes.;schema="
üìä Token usage (SIMULATED):
   - Model: simulated (SIMULATED)
   - Prompt tokens: 229
   - Completion tokens: 274
   - Total tokens: 503
üí∞ Estimated cost (simulated): $0.015090
‚úÖ session_summarize: Generated summary (1092 chars)
ü§ñ SIMULATED RESPONSE (SIMULATED)
Model: simulated
Mode: Development/Testing

SYSTEM: You are an expert session summarizer. Create a medium summary with comprehensive focus in a professional style.
USER: Please summarize this conversation session:

Demo conversation for session demo_session
User: Hello, I'm testing the Lexon language features.
Assistant: Great! Lexon is a revolutionary LLM-native programming language.
User: Can you explain the key features?
Assistant: Lexon supports async/await, multimodal requests, session management, and anti-hallucination validation.
User: That sounds impressive. How does the session system work?
Assistant: Sessions allow persistent context across multiple interactions, with automatic summarization and compression.
User: Perfect, let's test the context window management now.

Summary requirements:
- Length: medium
- Focus: comprehensive
- Style: professional

Provide a comprehensive yet concise summary that captures the key points, decisions, and outcomes.

RESPONSE: This is a simulated LLM response for development and testing purposes.

üß† context_window_manage: Managing context window for session 'demo_session'
‚úÖ context_window_manage: No compression needed (143 tokens < 4000 limit)

